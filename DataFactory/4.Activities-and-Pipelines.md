# Activities and Pipelines in Azure Data Factory

## Pipeline Concepts
- Container for activities that form a workflow unit
- Activities in a pipeline can be chained together
- Can be triggered manually or automatically
- Supports parameters for flexible execution

## Activity Types

### 1. Data Movement Activities
- **Copy Activity**: Primary data movement activity
  ```json
  {
      "name": "CopyFromBlobToSQL",
      "type": "Copy",
      "inputs": ["AzureBlobDataset"],
      "outputs": ["AzureSQLDataset"]
  }
  ```

### 2. Data Transformation Activities
- **Databricks Notebook**: Run Databricks notebooks
- **HDInsight Activities**: Run Hive, Pig, MapReduce jobs
- **Data Flow**: Visual data transformation designer
- **Machine Learning**: Score and batch prediction

### 3. Control Flow Activities
- **ForEach**: Iterate over a collection
- **If Condition**: Conditional branching
- **Until**: Loop until condition met
- **Wait**: Pause pipeline execution
- **Web Activity**: Call REST endpoints

## Pipeline Patterns

### Sequential Pipeline
```json
{
    "activities": [
        {
            "name": "CopyData",
            "type": "Copy",
            "dependsOn": []
        },
        {
            "name": "TransformData",
            "type": "DataFlow",
            "dependsOn": [
                {
                    "activity": "CopyData",
                    "dependencyConditions": ["Succeeded"]
                }
            ]
        }
    ]
}
```

### Parallel Pipeline
```json
{
    "activities": [
        {
            "name": "CopyData1",
            "type": "Copy",
            "dependsOn": []
        },
        {
            "name": "CopyData2",
            "type": "Copy",
            "dependsOn": []
        }
    ]
}
```

## Best Practices
1. Use parameters for reusability
2. Implement error handling
3. Log pipeline runs for monitoring
4. Use triggers for automation
5. Implement incremental loading patterns